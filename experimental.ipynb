{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Scoring Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Credit scoring model to predict the creditworthiness of individuals based on historical financial data. Utilize classification algorithms and assess the model's accuracy.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data from CSV file\n",
    "dataset = pd.read_csv('Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 30)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset shape\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TARGET               0\n",
       "ID                   0\n",
       "DerogCnt             0\n",
       "CollectCnt           0\n",
       "BanruptcyInd         0\n",
       "InqCnt06             0\n",
       "InqTimeLast        188\n",
       "InqFinanceCnt24      0\n",
       "TLTimeFirst          0\n",
       "TLTimeLast           0\n",
       "TLCnt03              0\n",
       "TLCnt12              0\n",
       "TLCnt24              0\n",
       "TLCnt                3\n",
       "TLSum               40\n",
       "TLMaxSum            40\n",
       "TLSatCnt             4\n",
       "TLDel60Cnt           0\n",
       "TLBadCnt24           0\n",
       "TL75UtilCnt         99\n",
       "TL50UtilCnt         99\n",
       "TLBalHCPct          41\n",
       "TLSatPct             4\n",
       "TLDel3060Cnt24       0\n",
       "TLDel90Cnt24         0\n",
       "TLDel60CntAll        0\n",
       "TLOpenPct            3\n",
       "TLBadDerogCnt        0\n",
       "TLDel60Cnt24         0\n",
       "TLOpen24Pct          3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Discover any missing values\n",
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the strings to numeriacal data\n",
    "def clean_and_convert(value):\n",
    "    if isinstance(value, str):\n",
    "        value = value.replace('$', '').replace(',', '').replace('%', '').strip()\n",
    "        return float(value)\n",
    "    return value\n",
    "\n",
    "# Apply the cleaning function to specific columns\n",
    "columns_to_clean = [\"TLSum\", \"TLMaxSum\", \"TLBalHCPct\", \"TLSatPct\", \"TLOpenPct\", \"TLOpen24Pct\"]\n",
    "for col in columns_to_clean:\n",
    "    dataset[col] = dataset[col].apply(clean_and_convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replance the missing values with the mean of the column\n",
    "dataset = dataset.fillna(dataset.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 30)\n"
     ]
    }
   ],
   "source": [
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Devide the data into features and target\n",
    "y = dataset.iloc[:, 0].values\n",
    "X = dataset.iloc[:, 1:28].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standerized the data \n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the classifier\n",
    "classifier = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "classifier.fit(X_train, y_train)\n",
    "prediction = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4- Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.835\n",
      "Confusion Matrix:\n",
      " [[492   6]\n",
      " [ 93   9]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.99      0.91       498\n",
      "           1       0.60      0.09      0.15       102\n",
      "\n",
      "    accuracy                           0.83       600\n",
      "   macro avg       0.72      0.54      0.53       600\n",
      "weighted avg       0.80      0.83      0.78       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", accuracy_score(y_test, prediction))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, prediction))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, prediction))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
